    use anyhow::Result;
use futures::stream::{self, StreamExt};
use std::collections::HashMap;

use crate::api::{ApiClient, OpenRouterRequest, RequestMessage};
use crate::config::Config;
use crate::database::{DatabaseManager, generate_word_cards, generate_grammar_cards};
use crate::models::*;
use crate::tts::{AzureTts, TtsConfig};

pub struct AnkiCreator {
    api_client: ApiClient,
    db_manager: DatabaseManager,
    pub config: Config,
}

impl AnkiCreator {
    pub async fn new(config: Config) -> Result<Self> {
        let api_client = ApiClient::new(config.clone())?;
        let db_manager = DatabaseManager::new(config.clone()).await?;
        
        Ok(AnkiCreator {
            api_client,
            db_manager,
            config,
        })
    }

    // æ›´æ–°æ‰€æœ‰å•è¯çš„è¯æ€§
    pub async fn update_all_word_parts_of_speech(&self) -> Result<()> {
        println!("ğŸ”„ å¼€å§‹æ›´æ–°æ‰€æœ‰å•è¯çš„è¯æ€§...");
        
        // è·å–æ‰€æœ‰å•è¯è®°å½•
        let words = self.db_manager.get_all_words().await?;
        
        if words.is_empty() {
            println!("âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å•è¯");
            return Ok(());
        }
        
        println!("ğŸ“Š æ‰¾åˆ° {} ä¸ªå•è¯éœ€è¦æ›´æ–°è¯æ€§", words.len());
        
        // ä½¿ç”¨å¹¶å‘æµå¤„ç†æ‰€æœ‰å•è¯
        let semaphore = std::sync::Arc::new(tokio::sync::Semaphore::new(self.config.processing.concurrent_requests));
        
        let total_words = words.len();
        let update_results: Result<Vec<()>, anyhow::Error> = stream::iter(words.into_iter().enumerate())
            .map(|(i, word)| {
                let semaphore = semaphore.clone();
                let api_client = &self.api_client;
                let db_manager = &self.db_manager;
                async move {
                    let _permit = semaphore.acquire().await.unwrap();
                    
                    println!("  ğŸ” æ›´æ–°å•è¯ {}/{}: {} ({})", 
                        i + 1, total_words, word.word, word.kana);
                    
                    // é‡æ–°åˆ†æå•è¯ä»¥è·å–æœ€æ–°çš„è¯æ€§
                    let prompt = format!(r#"
è¯·é‡æ–°åˆ†æè¿™ä¸ªæ—¥è¯­å•è¯çš„è¯æ€§ï¼Œåªéœ€è¦è¿”å›å‡†ç¡®çš„è¯æ€§ä¿¡æ¯ï¼š

å•è¯ï¼š{}
å‡åï¼š{}
éŸ³è°ƒï¼š{}

è¯·ç”¨ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼ˆåªéœ€è¦è¯æ€§ä¿¡æ¯ï¼‰ï¼š
{{
  "part_of_speech": ["è¯æ€§1", "è¯æ€§2"]
}}

é‡è¦è¯æ€§æ ‡æ³¨è§„åˆ™ï¼š
1. åŠ¨è¯å¿…é¡»æ˜ç¡®æ ‡æ³¨ä¸º"è‡ªåŠ¨è¯"æˆ–"ä»–åŠ¨è¯"ï¼Œä¸è¦åªå†™"åŠ¨è¯"
2. å¦‚æœä¸€ä¸ªè¯æ—¢æ˜¯è‡ªåŠ¨è¯åˆæ˜¯ä»–åŠ¨è¯ï¼Œå°±æ ‡æ³¨["è‡ªåŠ¨è¯", "ä»–åŠ¨è¯"]
3. å½¢å®¹è¯åˆ†ä¸º"ä¸€ç±»å½¢å®¹è¯"å’Œ"äºŒç±»å½¢å®¹è¯"
4. ä¸€å¾‹ä½¿ç”¨ç®€ä½“ä¸­æ–‡è¯æ€§æ ‡æ³¨ï¼šåè¯ã€è‡ªåŠ¨è¯ã€ä»–åŠ¨è¯ã€ä¸€ç±»å½¢å®¹è¯ã€äºŒç±»å½¢å®¹è¯ã€å‰¯è¯ã€è¿è¯ã€åŠ©è¯ã€æ„Ÿå¹è¯ç­‰
5. ä¸è¦å‡ºç°"å‹•è©ï½œè‡ªå‹•è©"è¿™ç§é‡å¤æ ‡æ³¨
6. åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹
"#, word.word, word.kana, word.pitch);

                    let request = OpenRouterRequest {
                        model: self.config.api.models.word_analysis_model.clone(),
                        messages: vec![RequestMessage {
                            role: "user".to_string(),
                            content: prompt,
                        }],
                        max_tokens: 1000,
                        temperature: 0.1,
                    };

                    match api_client.make_request_with_retry(&request).await {
                        Ok(api_response) => {
                            let content = &api_response.choices[0].message.content;
                            
                            // æå–JSONéƒ¨åˆ†
                            let json_start = content.find('{').unwrap_or(0);
                            let json_end = content.rfind('}').map(|i| i + 1).unwrap_or(content.len());
                            let json_content = &content[json_start..json_end];
                            
                            // è§£æè¯æ€§ç»“æœ
                            match serde_json::from_str::<serde_json::Value>(json_content) {
                                Ok(parsed) => {
                                    if let Some(pos_array) = parsed.get("part_of_speech").and_then(|v| v.as_array()) {
                                        let new_parts_of_speech: Vec<String> = pos_array
                                            .iter()
                                            .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                            .collect();
                                        
                                        if !new_parts_of_speech.is_empty() {
                                            let new_pos_str = new_parts_of_speech.join("ï½œ");
                                            
                                            // æ£€æŸ¥è¯æ€§æ˜¯å¦æœ‰å˜åŒ–
                                            if word.part_of_speech != new_pos_str {
                                                println!("    ğŸ”„ è¯æ€§æ›´æ–°: {} -> {}", 
                                                    word.part_of_speech, new_pos_str);
                                                
                                                // æ›´æ–°æ•°æ®åº“ä¸­çš„è¯æ€§
                                                if let Err(e) = db_manager.update_word_part_of_speech(word.id, &new_pos_str).await {
                                                    println!("    âŒ æ›´æ–°å¤±è´¥: {}", e);
                                                } else {
                                                    println!("    âœ… æ›´æ–°æˆåŠŸ");
                                                }
                                            } else {
                                                println!("    âœ… è¯æ€§æ— å˜åŒ–ï¼Œè·³è¿‡æ›´æ–°");
                                            }
                                        } else {
                                            println!("    âš ï¸  æœªèƒ½è§£æåˆ°æœ‰æ•ˆè¯æ€§");
                                        }
                                    } else {
                                        println!("    âš ï¸  å“åº”æ ¼å¼ä¸æ­£ç¡®");
                                    }
                                },
                                Err(e) => {
                                    println!("    âŒ JSONè§£æå¤±è´¥: {}", e);
                                }
                            }
                        },
                        Err(e) => {
                            println!("    âŒ APIè¯·æ±‚å¤±è´¥: {}", e);
                        }
                    }
                    
                    // æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    tokio::time::sleep(tokio::time::Duration::from_millis(
                        self.config.processing.request_delay_ms
                    )).await;
                    
                    Ok(())
                }
            })
            .buffer_unordered(self.config.processing.concurrent_requests)
            .collect::<Vec<Result<(), anyhow::Error>>>()
            .await
            .into_iter()
            .collect();
        
        update_results?;
        
        println!("ğŸ‰ æ‰€æœ‰å•è¯è¯æ€§æ›´æ–°å®Œæˆï¼");
        Ok(())
    }

    // æ›´æ–°æ‰€æœ‰å•è¯çš„è§£æ
    pub async fn update_all_word_analysis(&self) -> Result<()> {
        println!("ğŸ”„ å¼€å§‹æ›´æ–°æ‰€æœ‰å•è¯çš„è§£æ...");
        
        // è·å–æ‰€æœ‰å•è¯è®°å½•
        let words = self.db_manager.get_all_words().await?;
        
        if words.is_empty() {
            println!("âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å•è¯");
            return Ok(());
        }
        
        println!("ğŸ“Š æ‰¾åˆ° {} ä¸ªå•è¯éœ€è¦æ›´æ–°è§£æ", words.len());
        
        // ä½¿ç”¨å¹¶å‘æµå¤„ç†æ‰€æœ‰å•è¯
        let semaphore = std::sync::Arc::new(tokio::sync::Semaphore::new(self.config.processing.concurrent_requests));
        
        let total_words = words.len();
        let update_results: Result<Vec<()>, anyhow::Error> = stream::iter(words.into_iter().enumerate())
            .map(|(i, word)| {
                let semaphore = semaphore.clone();
                let analyzer = self;
                async move {
                    let _permit = semaphore.acquire().await.unwrap();
                    
                    println!("  ğŸ” æ›´æ–°å•è¯è§£æ {}/{}: {} ({})", 
                        i + 1, total_words, word.word, word.kana);
                    
                    // å¤ç”¨ç°æœ‰çš„åˆ†æé€»è¾‘
                    let parts_of_speech: Vec<&str> = word.part_of_speech.split('ï½œ').collect();
                    let parts_of_speech_vec: Vec<String> = parts_of_speech.iter().map(|s| s.to_string()).collect();
                    
                    let basic_word = BasicWordInfo {
                        word: word.word.clone(),
                        kana: word.kana.clone(),
                        pitch: word.pitch.clone(),
                        part_of_speech: parts_of_speech_vec.clone(),
                    };
                    
                    match analyzer.analyze_word_with_multiple_pos(&basic_word, &parts_of_speech_vec).await {
                        Ok(new_analysis) => {
                            // æ£€æŸ¥è§£ææ˜¯å¦æœ‰å˜åŒ–
                            if word.analysis != new_analysis {
                                println!("    ğŸ”„ è§£ææ›´æ–°: é•¿åº¦ {} -> {}", 
                                    word.analysis.len(), new_analysis.len());
                                
                                // æ›´æ–°æ•°æ®åº“ä¸­çš„è§£æ
                                if let Err(e) = analyzer.db_manager.update_word_analysis(word.id, &new_analysis).await {
                                    println!("    âŒ æ›´æ–°å¤±è´¥: {}", e);
                                } else {
                                    println!("    âœ… æ›´æ–°æˆåŠŸ");
                                }
                            } else {
                                println!("    âœ… è§£ææ— å˜åŒ–ï¼Œè·³è¿‡æ›´æ–°");
                            }
                        },
                        Err(e) => {
                            println!("    âŒ åˆ†æå¤±è´¥: {}", e);
                        }
                    }
                    
                    // æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    tokio::time::sleep(tokio::time::Duration::from_millis(
                        analyzer.config.processing.request_delay_ms
                    )).await;
                    
                    Ok(())
                }
            })
            .buffer_unordered(self.config.processing.concurrent_requests)
            .collect::<Vec<Result<(), anyhow::Error>>>()
            .await
            .into_iter()
            .collect();
        
        update_results?;
        
        println!("ğŸ‰ æ‰€æœ‰å•è¯è§£ææ›´æ–°å®Œæˆï¼");
        Ok(())
    }

    // æ ¹æ®IDæ›´æ–°å•è¯è§£æ
    pub async fn update_word_analysis_by_id(&self, id: i64) -> Result<()> {
        println!("ğŸ”„ å¼€å§‹æ ¹æ®IDæ›´æ–°å•è¯è§£æ...");
        
        // è·å–æŒ‡å®šIDçš„å•è¯
        let word = match self.db_manager.get_word_by_id(id).await? {
            Some(word) => word,
            None => {
                println!("âŒ æœªæ‰¾åˆ°IDä¸º {} çš„å•è¯", id);
                return Ok(());
            }
        };
        
        println!("ğŸ“ æ‰¾åˆ°å•è¯: {} ({}) - {}", word.word, word.kana, word.part_of_speech);
        
        // å¤ç”¨ç°æœ‰çš„åˆ†æé€»è¾‘
        let parts_of_speech: Vec<&str> = word.part_of_speech.split('ï½œ').collect();
        let parts_of_speech_vec: Vec<String> = parts_of_speech.iter().map(|s| s.to_string()).collect();
        
        let basic_word = BasicWordInfo {
            word: word.word.clone(),
            kana: word.kana.clone(),
            pitch: word.pitch.clone(),
            part_of_speech: parts_of_speech_vec.clone(),
        };
        
        match self.analyze_word_with_multiple_pos(&basic_word, &parts_of_speech_vec).await {
            Ok(new_analysis) => {
                // æ£€æŸ¥è§£ææ˜¯å¦æœ‰å˜åŒ–
                if word.analysis != new_analysis {
                    println!("ğŸ”„ è§£ææ›´æ–°: é•¿åº¦ {} -> {}", 
                        word.analysis.len(), new_analysis.len());
                    
                    // æ›´æ–°æ•°æ®åº“ä¸­çš„è§£æ
                    self.db_manager.update_word_analysis(id, &new_analysis).await?;
                    println!("âœ… å•è¯è§£ææ›´æ–°æˆåŠŸ");
                } else {
                    println!("âœ… è§£ææ— å˜åŒ–ï¼Œè·³è¿‡æ›´æ–°");
                }
            },
            Err(e) => {
                println!("âŒ åˆ†æå¤±è´¥: {}", e);
                return Err(e);
            }
        }
        
        Ok(())
    }

    // ç¬¬ä¸€æ­¥ï¼šæå–å•è¯å’Œè¯­æ³•çš„åŸºæœ¬ä¿¡æ¯
    pub async fn extract_words_and_grammar(&self, text: &str) -> Result<ExtractionResult> {
        let prompt = format!(r#"
è¯·åˆ†æä»¥ä¸‹æ—¥è¯­æ–‡æœ¬ï¼Œæå–å‡ºæ‰€æœ‰å•è¯å’Œè¯­æ³•ç‚¹çš„åŸºæœ¬ä¿¡æ¯ï¼š

1. å•è¯éƒ¨åˆ†ï¼š
   - å°†æ‰€æœ‰å•è¯è½¬æ¢ä¸ºè¾ä¹¦å½¢ï¼ˆåŸå½¢ï¼‰
   - æä¾›å‡åè¯»éŸ³
   - æä¾›éŸ³è°ƒï¼ˆç”¨0-9æ•°å­—è¡¨ç¤ºï¼‰
   - ç¡®å®šè¯æ€§ï¼ˆè¯·ç²¾ç¡®æ ‡æ³¨ï¼‰

2. è¯­æ³•éƒ¨åˆ†ï¼š
   - è¯†åˆ«è¯­æ³•ç»“æ„å’Œè¡¨è¾¾æ–¹å¼
   - æä¾›å‡åè¯»éŸ³

é‡è¦è¯æ€§æ ‡æ³¨è§„åˆ™ï¼š
- åŠ¨è¯å¿…é¡»æ˜ç¡®æ ‡æ³¨ä¸º"è‡ªåŠ¨è¯"æˆ–"ä»–åŠ¨è¯"ï¼Œä¸è¦åªå†™"åŠ¨è¯"
- å¦‚æœä¸€ä¸ªè¯æ—¢æ˜¯è‡ªåŠ¨è¯åˆæ˜¯ä»–åŠ¨è¯ï¼Œå°±æ ‡æ³¨["è‡ªåŠ¨è¯", "ä»–åŠ¨è¯"]
- å½¢å®¹è¯åˆ†ä¸º"ä¸€ç±»å½¢å®¹è¯"å’Œ"äºŒç±»å½¢å®¹è¯"
- ä¸€å¾‹ä½¿ç”¨ç®€ä½“ä¸­æ–‡è¯æ€§ï¼šåè¯ã€è‡ªåŠ¨è¯ã€ä»–åŠ¨è¯ã€ä¸€ç±»å½¢å®¹è¯ã€äºŒç±»å½¢å®¹è¯ã€å‰¯è¯ã€è¿è¯ã€åŠ©è¯ã€æ„Ÿå¹è¯ç­‰
- ä¸è¦å‡ºç°é‡å¤æ ‡æ³¨å¦‚"å‹•è©ï½œè‡ªå‹•è©"

è¯·ç”¨ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼ˆåªéœ€è¦åŸºæœ¬ä¿¡æ¯ï¼Œä¸éœ€è¦è¯¦ç»†è§£é‡Šï¼‰ï¼š
{{
  "words": [
    {{
      "word": "å˜è¯­è¾ä¹¦å½¢",
      "kana": "ã‹ãª",
      "pitch": "0",
      "part_of_speech": ["åè¯", "ä»–åŠ¨è¯"]
    }}
  ],
  "grammar": [
    {{
      "grammar": "è¯­æ³•è¡¨è¾¾",
      "kana": "ã‹ãª"
    }}
  ]
}}

è¦åˆ†æçš„æ–‡æœ¬ï¼š
{}
"#, text);

        let request = OpenRouterRequest {
            model: self.config.api.models.extraction_model.clone(),
            messages: vec![RequestMessage {
                role: "user".to_string(),
                content: prompt,
            }],
            max_tokens: 100000,
            temperature: 0.1,
        };

        let api_response = self.api_client.make_request_with_retry(&request).await?;
        let content = &api_response.choices[0].message.content;
        
        // æå–JSONéƒ¨åˆ†
        let json_start = content.find('{').unwrap_or(0);
        let json_end = content.rfind('}').map(|i| i + 1).unwrap_or(content.len());
        let json_content = &content[json_start..json_end];
        
        // è§£ææå–ç»“æœ
        let extraction: ExtractionResult = serde_json::from_str(json_content)
            .map_err(|e| anyhow::anyhow!("è§£ææå–ç»“æœå¤±è´¥: {}\nå“åº”å†…å®¹: {}", e, json_content))?;
        
        Ok(extraction)
    }

    // ç¬¬äºŒæ­¥ï¼šè¯¦ç»†åˆ†æå•ä¸ªå•è¯ï¼ˆæ”¯æŒå¤šè¯æ€§ï¼‰
    pub async fn analyze_word_with_multiple_pos(&self, word: &BasicWordInfo, parts_of_speech: &[String]) -> Result<String> {
        let pos_list = parts_of_speech.join("ã€");
        let prompt = format!(r#"
è¯·åˆ†æè¿™ä¸ªæ—¥è¯­å•è¯çš„ç”¨æ³•ï¼Œä»¥çº¯HTMLæ ¼å¼å›å¤ï¼Œå‚è€ƒä»¥ä¸‹ç¤ºä¾‹æ ¼å¼ï¼š

ç¤ºä¾‹ï¼ˆå•è¯ï¼šå¸¯ï¼Œå‡åï¼šãŠã³ï¼ŒéŸ³è°ƒï¼šobiï¼Œè¯æ€§ï¼šåè¯ï¼‰ï¼š
<div>ã€Œå¸¯ã€ï¼ˆãŠã³ã€obiï¼‰æ˜¯ä¸€ä¸ªæ—¥è¯­åè¯ï¼Œæ„æ€æ˜¯<b>"è…°å¸¦"ã€"å¸¦å­"æˆ–"åœ°å¸¦"</b>ã€‚å®ƒæ˜¯ä¸€ä¸ªéå¸¸é€šç”¨çš„è¯ï¼Œæ ¹æ®ä¸åŒçš„è¯­å¢ƒæœ‰ä¸åŒçš„å«ä¹‰ï¼Œä½†æ ¸å¿ƒéƒ½ä¸"å¸¦çŠ¶ç‰©"æˆ–"åŒºåŸŸ"æœ‰å…³ã€‚</div>
<hr>
<div>1. æœé¥°ä¸Šçš„"è…°å¸¦" ğŸ‘˜<br>
è¿™æ˜¯æœ€å¸¸è§ã€æœ€æ ¸å¿ƒçš„ç”¨æ³•ã€‚ç‰¹æŒ‡ç³»åœ¨å’Œæœã€æµ´è¡£ç­‰ä¼ ç»Ÿæ—¥æœ¬æœé¥°ä¸Šçš„å®½è…°å¸¦ã€‚<br>
ä¾‹ï¼š å¸¯ã‚’ç· ã‚ã‚‹ (obi o shimeru) - ç³»è…°å¸¦ã€‚<br>
ä¾‹ï¼š ç€ç‰©ã¨å¸¯ (kimono to obi) - å’Œæœå’Œè…°å¸¦ã€‚<br><br>
2. "åœ°å¸¦"ã€"åŒºåŸŸ" ğŸ—ºï¸<br>
å¸¦æœ‰æ¯”å–»è‰²å½©ï¼ŒæŒ‡æŸä¸ªå…·æœ‰ç‰¹å®šç‰¹å¾çš„å¸¦çŠ¶åŒºåŸŸã€‚<br>
ä¾‹ï¼š å°é¢¨ã®å¸¯ (taifÅ« no obi) - å°é£å¸¦ã€‚<br>
ä¾‹ï¼š ç«å±±å¸¯ (kazan tai) - ç«å±±å¸¦ã€‚</div>
<hr>
<div>ã€Œå¸¯ã€è¿™ä¸ªæ±‰å­—æœ¬èº«å°±å¸¦æœ‰<b>"æŸç¼š"ã€"æ†ç»‘"æˆ–"å¸¦çŠ¶"</b>çš„å«ä¹‰ã€‚åœ¨æ—¥è¯­ä¸­ï¼Œå®ƒå®Œç¾åœ°ä¿ç•™äº†è¿™äº›æ ¸å¿ƒæ¦‚å¿µï¼Œä»å…·ä½“çš„æœé¥°è…°å¸¦åˆ°æŠ½è±¡çš„åœ°ç†åŒºåŸŸï¼Œéƒ½ç”¨è¿™ä¸ªè¯æ¥è¡¨è¾¾ã€‚ä¸åŒè¯­å¢ƒä¸‹ï¼Œé‡ç‚¹ä¼šä»å…·ä½“çš„ç‰©ç†å¯¹è±¡è½¬å‘æŠ½è±¡çš„æ¦‚å¿µæ€§åŒºåŸŸã€‚</div>
<hr>
<div>æ€»çš„æ¥è¯´ï¼Œã€Œå¸¯ã€çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯<b>"å¸¦çŠ¶ç‰©"æˆ–"å¸¦çŠ¶åŒºåŸŸ"</b>ï¼Œå®ƒå¯ä»¥æŒ‡å®é™…çš„ç‰©å“ï¼Œä¹Ÿå¯ä»¥æŒ‡æŠ½è±¡çš„æ¦‚å¿µã€‚ä¸åŒè¯­å¢ƒä¸‹ï¼Œé‡ç‚¹ä¼šä»å…·ä½“çš„ç‰©ç†å¯¹è±¡è½¬å‘æŠ½è±¡çš„æ¦‚å¿µæ€§åŒºåŸŸã€‚</div>
<hr>
<div><b>è¯æ±‡å¯¹æ¯”ï¼š</b><br><br>
<b>ã€Œå¸¯ã€vsã€Œãƒ™ãƒ«ãƒˆã€ï¼š</b> ã€Œãƒ™ãƒ«ãƒˆã€æ˜¯å¤–æ¥è¯ï¼Œå¤šæŒ‡ç°ä»£æœé¥°çš„çš®å¸¦ï¼Œè€Œã€Œå¸¯ã€æ›´åå‘ä¼ ç»Ÿæ–‡åŒ–ï¼Œå¦‚å’Œæœè…°å¸¦ã€‚<br><br>
<b>ã€Œå¸¯ã€vsã€Œç´ã€ï¼š</b> ã€Œç´ã€é€šå¸¸æŒ‡ç»†ç»³ã€ç»†å¸¦ï¼Œã€Œå¸¯ã€åˆ™æŒ‡è¾ƒå®½çš„å¸¦çŠ¶ç‰©ï¼Œä¸”æ›´æ­£å¼ã€‚<br><br>
<b>ã€Œå¸¯ã€vsã€Œãƒãƒ³ãƒ‰ã€ï¼š</b> ã€Œãƒãƒ³ãƒ‰ã€å¤šç”¨äºæŠ€æœ¯æˆ–åŒ»ç–—é¢†åŸŸï¼ˆå¦‚é¢‘æ®µã€ç»·å¸¦ï¼‰ï¼Œã€Œå¸¯ã€æ›´å¤šç”¨äºåœ°ç†å’Œæœé¥°é¢†åŸŸã€‚</div>

ç°åœ¨è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼åˆ†æï¼š

å•è¯ï¼š{}
å‡åï¼š{}
éŸ³è°ƒï¼š{}
è¯æ€§ï¼š{}

é‡è¦äº‹é¡¹ï¼š
1. ç›´æ¥å›å¤HTMLå†…å®¹ï¼Œä¸è¦ä½¿ç”¨markdownä»£ç å—æ ¼å¼
2. ä¸è¦æ·»åŠ ```html```æ ‡è®°
3. ç²—ä½“ä½¿ç”¨<b></b>æ ‡ç­¾ï¼Œç»ä¸è¦ä½¿ç”¨**ç¬¦å·
4. å¦‚æœæœ‰å¤šä¸ªè¯æ€§ï¼Œè¯·å…¨é¢åˆ†ææ‰€æœ‰è¯æ€§çš„ç”¨æ³•
5. ä¸è¦é‡å¤æ¨¡æ¿åŒ–çš„æ ‡é¢˜
"#, word.word, word.kana, word.pitch, pos_list);

        let request = OpenRouterRequest {
            model: self.config.api.models.word_analysis_model.clone(),
            messages: vec![RequestMessage {
                role: "user".to_string(),
                content: prompt,
            }],
            max_tokens: 100000,
            temperature: 0.1,
        };

        let api_response = self.api_client.make_request_with_retry(&request).await?;
        let analysis = &api_response.choices[0].message.content.trim();
        
        Ok(analysis.to_string())
    }

    // ç¬¬äºŒæ­¥ï¼šè¯¦ç»†åˆ†æå•ä¸ªè¯­æ³•
    pub async fn analyze_grammar(&self, grammar: &BasicGrammarInfo) -> Result<String> {
        let prompt = format!(r#"
è¯·è¯¦ç»†åˆ†æè¿™ä¸ªæ—¥è¯­è¯­æ³•ç‚¹ï¼š

è¯­æ³•ï¼š{}
å‡åï¼š{}

è¯·æä¾›ï¼š
1. è¯¦ç»†çš„ä¸­æ–‡è§£é‡Š
2. è¯­æ³•åŠŸèƒ½å’Œæ„ä¹‰
3. ä½¿ç”¨åœºåˆå’Œè¯­å¢ƒ
4. æ¥ç»­æ–¹æ³•ï¼ˆå‰åå¯ä»¥æ¥ä»€ä¹ˆï¼‰
5. ç”¨æ³•ä¾‹å¥å’Œæ³¨æ„ç‚¹
6. ç›¸ä¼¼è¯­æ³•çš„åŒºåˆ«

è¯·åªè¿”å›è¯¦ç»†çš„ä¸­æ–‡åˆ†æå†…å®¹ï¼Œä¸éœ€è¦JSONæ ¼å¼ã€‚
"#, grammar.grammar, grammar.kana);

        let request = OpenRouterRequest {
            model: self.config.api.models.grammar_analysis_model.clone(),
            messages: vec![RequestMessage {
                role: "user".to_string(),
                content: prompt,
            }],
            max_tokens: 100000,
            temperature: 0.1,
        };

        let api_response = self.api_client.make_request_with_retry(&request).await?;
        let analysis = &api_response.choices[0].message.content.trim();
        
        Ok(analysis.to_string())
    }

    // ç”Ÿæˆå•è¯ Anki å¡ç‰‡
    pub async fn generate_word_cards(&self) -> Result<()> {
        let words = self.db_manager.get_all_words().await?;
        generate_word_cards(&words, &self.config.output.words_file)?;
        Ok(())
    }

    // ç”Ÿæˆè¯­æ³• Anki å¡ç‰‡  
    pub async fn generate_grammar_cards(&self) -> Result<()> {
        let grammar = self.db_manager.get_all_grammar().await?;
        generate_grammar_cards(&grammar, &self.config.output.grammar_file)?;
        Ok(())
    }

    // åªå¤„ç†å•è¯çš„å‡½æ•°
    pub async fn process_words_only(&self, text: &str) -> Result<()> {
        let text_length = text.chars().count();
        println!("ğŸ“ è¾“å…¥æ–‡æœ¬é•¿åº¦: {} å­—ç¬¦", text_length);
        
        println!("ğŸ”„ ç¬¬ä¸€æ­¥ï¼šæå–å•è¯...");
        
        // ç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼Œä¸å†åˆ†å—
        let extraction = self.extract_words_and_grammar(text).await?;
        
        println!("ğŸ“ æ‰¾åˆ° {} ä¸ªå•è¯", extraction.words.len());

        println!("ğŸ”„ ç¬¬äºŒæ­¥ï¼šæŒ‰å•è¯åˆ†ç»„å¹¶æ£€æŸ¥é‡å¤...");
        
        // æŒ‰å•è¯ï¼ˆword+kana+pitchï¼‰åˆ†ç»„ï¼Œåˆå¹¶ç›¸åŒå•è¯çš„ä¸åŒè¯æ€§
        let mut word_groups: HashMap<(String, String, String), Vec<String>> = HashMap::new();
        
        for word in extraction.words.iter() {
            let key = (word.word.clone(), word.kana.clone(), word.pitch.clone());
            let group = word_groups.entry(key).or_insert_with(Vec::new);
            
            // åˆå¹¶è¯æ€§ï¼Œé¿å…é‡å¤
            for pos in &word.part_of_speech {
                if !group.contains(pos) {
                    group.push(pos.clone());
                }
            }
        }
        
        // æ£€æŸ¥å“ªäº›å•è¯å·²å­˜åœ¨ï¼Œå“ªäº›éœ€è¦åˆ†æ
        let mut words_to_analyze = Vec::new();
        let mut words_to_update: Vec<(i64, String, String)> = Vec::new();
        let mut skipped_count = 0;
        
        for ((word, kana, pitch), parts_of_speech) in word_groups.iter() {
            let exists = self.db_manager.check_word_exists(word, kana).await?;
            
            if exists {
                // è·å–å·²å­˜åœ¨çš„å•è¯ä¿¡æ¯
                if let Some(existing_word) = self.db_manager.get_existing_word_by_word_kana(word, kana).await? {
                    let new_pos_str = parts_of_speech.join("ï½œ");
                    
                    // æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° pitch æˆ–è¯æ€§
                    if existing_word.pitch != *pitch || existing_word.part_of_speech != new_pos_str {
                        println!("  ğŸ”„ æ›´æ–°å·²å­˜åœ¨å•è¯: {} ({}) - pitch: {}->{}, pos: {}->{}", 
                            word, kana, 
                            existing_word.pitch, pitch,
                            existing_word.part_of_speech, new_pos_str
                        );
                        
                        words_to_update.push((existing_word.id, pitch.clone(), new_pos_str));
                    } else {
                        skipped_count += 1;
                        println!("  âœ… è·³è¿‡å·²å­˜åœ¨çš„å•è¯ï¼ˆæ— å˜åŒ–ï¼‰: {} ({})", word, kana);
                    }
                } else {
                    // ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
                    skipped_count += 1;
                    println!("  âš ï¸ä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œè·³è¿‡å·²å­˜åœ¨çš„å•è¯: {} ({})", word, kana);
                }
            } else {
                let basic_word = BasicWordInfo {
                    word: word.clone(),
                    kana: kana.clone(),
                    pitch: pitch.clone(),
                    part_of_speech: parts_of_speech.clone(),
                };
                words_to_analyze.push((basic_word, parts_of_speech.clone()));
            }
        }

        println!("  è·³è¿‡ {} ä¸ªå·²å­˜åœ¨çš„å•è¯ï¼Œéœ€è¦æ›´æ–° {} ä¸ªå•è¯ï¼Œéœ€è¦åˆ†æ {} ä¸ªæ–°å•è¯", 
            skipped_count, 
            words_to_update.len(),
            words_to_analyze.len()
        );

        // å…ˆæ›´æ–°å·²å­˜åœ¨çš„å•è¯
        if !words_to_update.is_empty() {
            println!("ğŸ”„ æ›´æ–°å·²å­˜åœ¨å•è¯çš„ pitch å’Œè¯æ€§...");
            for (id, new_pitch, new_pos) in words_to_update {
                if let Err(e) = self.db_manager.update_word_pitch_and_pos(id, &new_pitch, &new_pos).await {
                    println!("  âŒ æ›´æ–°å¤±è´¥: ID {} - {}", id, e);
                } else {
                    println!("  âœ… æ›´æ–°æˆåŠŸ: ID {} - pitch: {}, pos: {}", id, new_pitch, new_pos);
                }
            }
        }
        
        // ä½¿ç”¨å¹¶å‘æµå¤„ç†æ‰€æœ‰å•è¯
        let semaphore = std::sync::Arc::new(tokio::sync::Semaphore::new(self.config.processing.concurrent_requests));
        
        let word_analyses_results: Result<Vec<Vec<WordAnalysis>>, anyhow::Error> = stream::iter(words_to_analyze.into_iter().enumerate())
            .map(|(i, (word, parts_of_speech))| {
                let semaphore = semaphore.clone();
                let analyzer = self;
                async move {
                    let _permit = semaphore.acquire().await.unwrap();
                    
                    let pos_display = parts_of_speech.join("ã€");
                    println!("  åˆ†æå•è¯ {}: {} ({})", i + 1, word.word, pos_display);
                    let analysis = analyzer.analyze_word_with_multiple_pos(&word, &parts_of_speech).await?;
                    
                    // ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºä¸€ä¸ªWordAnalysisè®°å½•ï¼Œæ‰€æœ‰è¯æ€§ç”¨ï½œåˆ†éš”
                    let merged_parts_of_speech = parts_of_speech.join("ï½œ");
                    let word_analysis = WordAnalysis {
                        word: word.word.clone(),
                        kana: word.kana.clone(),
                        pitch: word.pitch.clone(),
                        part_of_speech: merged_parts_of_speech,
                        analysis: analysis,
                    };
                    
                    Ok(vec![word_analysis])
                }
            })
            .buffer_unordered(10) // å…è®¸æœ€å¤š10ä¸ªå¹¶å‘ä»»åŠ¡
            .collect::<Vec<Result<Vec<WordAnalysis>, anyhow::Error>>>()
            .await
            .into_iter()
            .collect();
        
        // å±•å¹³ç»“æœ
        let new_word_analyses: Vec<WordAnalysis> = word_analyses_results?.into_iter().flatten().collect();

        println!("ğŸ’¾ ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“...");

        // ä¿å­˜æ–°åˆ†æçš„å•è¯åˆ°æ•°æ®åº“
        if !new_word_analyses.is_empty() {
            self.db_manager.save_words(&new_word_analyses).await?;
            println!("  âœ… ä¿å­˜äº† {} ä¸ªæ–°å•è¯åˆ°æ•°æ®åº“", new_word_analyses.len());
        } else {
            println!("  â„¹ï¸  æ²¡æœ‰æ–°å•è¯éœ€è¦ä¿å­˜");
        }

        println!("ğŸ“„ ç”Ÿæˆå•è¯ Anki å¡ç‰‡æ–‡ä»¶...");

        // ç”Ÿæˆå•è¯ Anki å¡ç‰‡
        self.generate_word_cards().await?;
        
        Ok(())
    }

    // åªå¤„ç†è¯­æ³•çš„å‡½æ•°
    pub async fn process_grammar_only(&self, text: &str) -> Result<()> {
        let text_length = text.chars().count();
        println!("ğŸ“ è¾“å…¥æ–‡æœ¬é•¿åº¦: {} å­—ç¬¦", text_length);
        
        println!("ğŸ”„ ç¬¬ä¸€æ­¥ï¼šæå–è¯­æ³•...");
        
        // ç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼Œä¸å†åˆ†å—
        let extraction = self.extract_words_and_grammar(text).await?;
        
        println!("ğŸ“ æ‰¾åˆ° {} ä¸ªè¯­æ³•ç‚¹", extraction.grammar.len());
        
        println!("ğŸ”„ ç¬¬äºŒæ­¥ï¼šå¹¶å‘è¯¦ç»†åˆ†ææ¯ä¸ªè¯­æ³•ç‚¹...");
        
        // ä½¿ç”¨å¹¶å‘å¤„ç†è¯­æ³•åˆ†æ
        let semaphore = std::sync::Arc::new(tokio::sync::Semaphore::new(self.config.processing.concurrent_requests));
        
        let grammar_analyses: Result<Vec<GrammarAnalysis>, anyhow::Error> = stream::iter(extraction.grammar.into_iter().enumerate())
            .map(|(i, grammar)| {
                let semaphore = semaphore.clone();
                let analyzer = self;
                async move {
                    let _permit = semaphore.acquire().await.unwrap();
                    
                    println!("  åˆ†æè¯­æ³• {}: {}", i + 1, grammar.grammar);
                    let analysis = analyzer.analyze_grammar(&grammar).await?;
                    
                    Ok(GrammarAnalysis {
                        grammar: grammar.grammar.clone(),
                        kana: grammar.kana.clone(),
                        analysis,
                    })
                }
            })
            .buffer_unordered(10)
            .collect::<Vec<Result<GrammarAnalysis, anyhow::Error>>>()
            .await
            .into_iter()
            .collect();
        
        let grammar_analyses = grammar_analyses?;

        println!("ğŸ’¾ ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“...");
        
        self.db_manager.save_grammar(&grammar_analyses).await?;
        
        println!("ğŸ“„ ç”Ÿæˆè¯­æ³• Anki å¡ç‰‡æ–‡ä»¶...");

        // ç”Ÿæˆè¯­æ³• Anki å¡ç‰‡
        self.generate_grammar_cards().await?;
        
        Ok(())
    }

    /// å¢é‡ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
    pub async fn generate_missing_audio_files(&self) -> Result<()> {
        println!("ğŸµ å¼€å§‹å¢é‡ç”ŸæˆéŸ³é¢‘æ–‡ä»¶...");
        
        // ç¡®ä¿éŸ³é¢‘ç›®å½•å­˜åœ¨
        std::fs::create_dir_all(&self.config.output.audio_dir)
            .map_err(|e| anyhow::anyhow!("æ— æ³•åˆ›å»ºéŸ³é¢‘ç›®å½• {}: {}", self.config.output.audio_dir, e))?;
        
        // è·å–æ‰€æœ‰å•è¯
        let words = self.db_manager.get_all_words().await?;
        
        if words.is_empty() {
            println!("âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å•è¯");
            return Ok(());
        }
        
        println!("ğŸ“Š æ‰¾åˆ° {} ä¸ªå•è¯ï¼Œæ£€æŸ¥ç¼ºå¤±çš„éŸ³é¢‘æ–‡ä»¶...", words.len());
        
        // åˆ›å»º TTS å®¢æˆ·ç«¯
        let tts_config = TtsConfig::from_config(&self.config.tts);
        let tts = AzureTts::new(tts_config);
        
        let mut missing_count = 0;
        let mut generated_count = 0;
        
        // ä½¿ç”¨å¹¶å‘æµå¤„ç†æ‰€æœ‰å•è¯
        let semaphore = std::sync::Arc::new(tokio::sync::Semaphore::new(self.config.processing.concurrent_requests));
        
        let total_words = words.len();
        let results: Vec<Result<(), anyhow::Error>> = stream::iter(words.into_iter().enumerate())
            .map(|(i, word)| {
                let semaphore = semaphore.clone();
                let tts = &tts;
                let audio_dir = &self.config.output.audio_dir;
                async move {
                    let _permit = semaphore.acquire().await.unwrap();
                    
                    let audio_filename = format!("japanese_word_{}.wav", word.id);
                    let audio_path = std::path::Path::new(audio_dir).join(&audio_filename);
                    
                    // æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if audio_path.exists() {
                        println!("  âœ… éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨: {} ({})", audio_filename, word.word);
                        return Ok(());
                    }
                    
                    println!("  ğŸµ ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ {}/{}: {} ({})", 
                        i + 1, total_words, audio_filename, word.kana);
                    
                    // ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨å‡åï¼ˆå‘éŸ³ï¼‰è€Œä¸æ˜¯æ±‰å­—
                    match tts.synthesize_text_to_file(&word.kana, &audio_path.to_string_lossy()).await {
                        Ok(_) => {
                            println!("  âœ… éŸ³é¢‘æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {}", audio_filename);
                            Ok(())
                        },
                        Err(e) => {
                            println!("  âŒ éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {} - {}", audio_filename, e);
                            Err(e)
                        }
                    }
                }
            })
            .buffer_unordered(self.config.processing.concurrent_requests)
            .collect::<Vec<_>>()
            .await;
        
        // ç»Ÿè®¡ç»“æœ
        for result in results {
            match result {
                Ok(_) => generated_count += 1,
                Err(_) => missing_count += 1,
            }
        }
        
        println!("\nğŸ‰ éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼");
        println!("   âœ… æˆåŠŸç”Ÿæˆ: {} ä¸ªéŸ³é¢‘æ–‡ä»¶", generated_count);
        if missing_count > 0 {
            println!("   âŒ ç”Ÿæˆå¤±è´¥: {} ä¸ªéŸ³é¢‘æ–‡ä»¶", missing_count);
        }
        println!("   ğŸ“ éŸ³é¢‘æ–‡ä»¶ç›®å½•: {}", self.config.output.audio_dir);
        
        Ok(())
    }


}
